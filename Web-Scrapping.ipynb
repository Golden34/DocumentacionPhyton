{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scrapping\n",
    "\n",
    "Web-Scrapping es la forma que tenemos para referirnos a la captura de información de cualquier sitio web. Su objetivo es capturar información de forma automática.\n",
    "\n",
    "La librería principal que vamos a utilizar es beautifulsoup. Para ello, hay que instalarla primero:\n",
    "En el símbolo del sistema:\n",
    "\n",
    ">pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También vamos a instalar request:\n",
    "\n",
    ">pip install requests\n",
    "\n",
    "Una vez instalado, se importa en el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso se divide principalmente en tres fases:\n",
    "\n",
    "- Carga de la dirección web a la que realizar el scrapping. A través de  requests.\n",
    "\n",
    "- Extracción del contenido de la web a partir de Beautifulsoup\n",
    "\n",
    "- Manipulación del contenido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de la URL\n",
    "\n",
    "Request necesita dos cosas: La URL y un elemento cabecera. El elemento cabecera es vital, ya que muchas web no permiten el acceso a la información a no ser que detecten que las peticiones vienen de un navegador. Por ejemplo Mozilla Firefox.\n",
    "\n",
    "Así, definimos headers como un diccionario cuyo User-Agent es Firefox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "# la url puede ser de cualquier página y será el objetivo de nuestro web-scrapper\n",
    "url = 'https://resultados.as.com/resultados/futbol/primera/2015_2016/jornada/regular_a_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = requests.get(url,headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos preguntar si la conexion con esa pagina web ha funcionado. Es decir si la propiedad status_code que devuelve re es 200 o 201. Si es así, pasamos a la siguiente fase.\n",
    "\n",
    "## 2. Extracción del contenido con BeautifulSoup\n",
    "\n",
    "Para ello, pasamos el contenido que ha obtenido re y que se encuentra en re.text a través de un parser de HTML en caso de que sea HTML u otro, en caso de que sea XML, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(re.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74286/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74281/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74279/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74280/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74278/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74282/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74277/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74284/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74285/\n",
      "/resultados/futbol/primera/2015_2016/directo/regular_a_1_74283/\n"
     ]
    }
   ],
   "source": [
    "if re.status_code == 200:\n",
    "    soup = BeautifulSoup( re.text, 'html.parser')\n",
    "    # print(soup.prettify())\n",
    "    #  y ahora preguntamos si se ha cargado algo de texto\n",
    "    if soup is not None:\n",
    "        # en articles vamos colocando todas las que cumplan tener la etiqueta li, y la clase\n",
    "        articles = soup.find_all('li',{'class' : 'list-resultado'})\n",
    "        for article in articles:\n",
    "            # y ahora iteraríamos para sacar todo lo necesario\n",
    "            # estadio\n",
    "            lugares = article.find_all('span',{'itemprop':'location'})\n",
    "            equipo = article.find('span',{'class' : 'nombre-equipo'}).getText()\n",
    "            enlaces = article.find_all('div', {'class':'cont-resultado finalizado'})\n",
    "            for enlace in enlaces:\n",
    "                direccion = enlace.find('a').get('href')\n",
    "                print(direccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podríamos ver el contenido de lo que ha capturado con esta instrucción:\n",
    "# print(soup.prettify())\n",
    "\n",
    "# o también mostrar la lista de etiquetas como una lista con el metodo children.\n",
    "# list(soup.children)\n",
    "\n",
    "# Cada elemento de la lista devuelto por la propiedad children también es un objeto BeautifulSoup, \n",
    "# por lo que también podemos activar el método html.children\n",
    "# con este, podemos encontrar a los hijos dentro de la etiqueta html:\n",
    "\n",
    "# list(html.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manipulación del contenido\n",
    "\n",
    "En esta fase vamos iterando sobre lo que hemos conseguido con BeautifulSoup y que lo vamos almacenando en objetos como articles. \n",
    "\n",
    "Si el contenido es directamente la información que se encuentra en lo encontrado por soup.find.all, sólo hay que imprimir o guardar.\n",
    "\n",
    "Sin embargo a veces hay que volver a llamar a otras búsquedas find, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # fecha y hora\n",
    "            masinfo= article.find_all('div',{'class' : 'info-evento'})\n",
    "            for info in masinfo:\n",
    "                fechahora = info.find('time').get('content')\n",
    "                # una vez aquí ya tenemos contenido real\n",
    "                print(fechahora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el anterior código, vemos que para obtener la fecha y la hora, simplemente utilizamos find. Este es el caso cuando no hay una lista de elementos. Suele devolver un dato ya disponible, no un objeto iterable.\n",
    "\n",
    "Otra alternativa que podemos probar cuando este get no funcione porque necesitemos especificar la clase es:\n",
    "\n",
    "    resultado = enlace.find('a',{'class' : 'resultado'}).getText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El proceso de Web-Scrapping no es un proceso complicado, pero si tedioso.\n",
    "Y es tedioso porque hay que comprender cuál es la estructura de la web que queremos scrappear y es posible que con el tiempo, un web-scrapper que funcionaba, no nos funciona actualmente por haber cambiado la estructura de la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
